{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - 标量与矢量运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use for loop to calc, cost : 7.745594\n"
     ]
    }
   ],
   "source": [
    "# 测试tensor加法的运算效率\n",
    "\n",
    "size = 1024 ** 2\n",
    "\n",
    "x = torch.ones(size)\n",
    "y = torch.ones(size)\n",
    "z = torch.zeros(size)\n",
    "\n",
    "start = time()\n",
    "for i in range(size):\n",
    "    z[i] = x[i] + y[i]\n",
    "print(\"use for loop to calc, cost : %f\"%(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use vector add to calc, cost : 0.001416\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "z = x + y\n",
    "print(\"use vector add to calc, cost : %f\"%(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试使用ｃｕｄａ加速后的矢量运算效率\n",
    "a = torch.ones(size)\n",
    "b = torch.ones(size)\n",
    "c = torch.zeros(size)\n",
    "start = time()\n",
    "c = a + b\n",
    "print(\"cudause vector add to calc, cost : %f\"%(time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - 手工实现线性回归算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1670, 0.1013, 0.3756]) tensor(0.6782)\n"
     ]
    }
   ],
   "source": [
    "# 原文为两个变量，此处测试三变量\n",
    "num_inputs = 3\n",
    "num_examples = 1000\n",
    "true_w = [2.05, -3.4, 1.28]\n",
    "true_b = 0.2\n",
    "features = torch.randn(num_examples, num_inputs,\n",
    "                       dtype=torch.float32)\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1]  \\\n",
    "            + true_w[2] * features[:, 2]+ true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),\n",
    "                       dtype=torch.float32)\n",
    "\n",
    "print(features[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5778, -1.2826, -0.6243],\n",
      "        [ 0.9409, -1.7390, -0.3835],\n",
      "        [ 0.1676, -0.9237, -1.5869],\n",
      "        [-1.0868, -0.4981, -0.9275],\n",
      "        [ 0.0693, -0.3614,  0.7760],\n",
      "        [-0.4783,  1.2052,  0.6201],\n",
      "        [ 0.2797, -0.6210, -0.5013],\n",
      "        [ 0.3057, -0.0477,  1.0127],\n",
      "        [-0.1686,  0.3785, -0.3210],\n",
      "        [ 0.5045, -0.9925,  0.4847]]) tensor([ 4.9342,  7.5444,  1.6726, -1.5184,  2.5661, -4.0903,  2.2346,  2.3030,\n",
      "        -1.8396,  5.2212])\n"
     ]
    }
   ],
   "source": [
    "# 本函数已保存在d2lzh包中方便以后使用\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)  # 样本的读取顺序是随机的\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # 最后一次可能不足一个batch\n",
    "        yield  features.index_select(0, j), labels.index_select(0, j)\n",
    "\n",
    "        \n",
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  # 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "    return torch.mm(X, w) + b\n",
    "\n",
    "def squared_loss(y_hat, y):  # 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "    # 注意这里返回的是向量, 另外, pytorch里的MSELoss并没有除以 2\n",
    "    return (y_hat - y.view(y_hat.size())) ** 2 / 2\n",
    "\n",
    "def sgd(params, lr, batch_size):  # 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.138208\n",
      "epoch 2, loss 0.146106\n",
      "epoch 3, loss 0.019103\n",
      "epoch 4, loss 0.002582\n",
      "epoch 5, loss 0.000390\n",
      "epoch 6, loss 0.000098\n",
      "epoch 7, loss 0.000057\n",
      "epoch 8, loss 0.000051\n",
      "epoch 9, loss 0.000050\n",
      "epoch 10, loss 0.000050\n",
      "epoch 11, loss 0.000050\n",
      "epoch 12, loss 0.000050\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32)\n",
    "b = torch.zeros(1, dtype=torch.float32)\n",
    "\n",
    "w.requires_grad_(requires_grad=True)\n",
    "b.requires_grad_(requires_grad=True) \n",
    "\n",
    "lr = 0.01\n",
    "num_epochs = 12\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要num_epochs个迭代周期\n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X\n",
    "    # 和y分别是小批量样本的特征和标签\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y).sum()  # l是有关小批量X和y的损失\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度\n",
    "        sgd([w, b], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数\n",
    "\n",
    "        # 不要忘了梯度清零\n",
    "        w.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real weights: \n",
      " [2.05, -3.4, 1.28] \n",
      "** \n",
      "calculated weights: \n",
      " tensor([[ 2.0496],\n",
      "        [-3.3998],\n",
      "        [ 1.2792]], requires_grad=True)\n",
      "<------>\n",
      "<------>\n",
      "real bias: \n",
      " 0.2 \n",
      "** \n",
      "calculated bias: \n",
      " tensor([0.2006], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('real weights: \\n', true_w, '\\n** \\ncalculated weights: \\n', w)\n",
    "print('<------>\\n<------>\\nreal bias: \\n', true_b, '\\n** \\ncalculated bias: \\n', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - 用torch API简洁实现线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1314, -0.0537, -2.0535]) tensor(-4.5778)\n"
     ]
    }
   ],
   "source": [
    "# 原文为两个变量，此处测试三变量\n",
    "num_inputs = 3\n",
    "num_examples = 1000\n",
    "true_w = [2.05, -3.4, 1.28]\n",
    "true_b = 0.2\n",
    "features = torch.randn(num_examples, num_inputs,\n",
    "                       dtype=torch.float32)\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1]  \\\n",
    "            + true_w[2] * features[:, 2]+ true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),\n",
    "                       dtype=torch.float32)\n",
    "\n",
    "print(features[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.3551e-01, -1.4299e+00,  1.0157e+00],\n",
      "        [ 1.3555e+00, -7.1962e-01, -3.1199e-01],\n",
      "        [-1.5121e+00, -3.6046e-01, -1.4631e+00],\n",
      "        [ 8.4685e-01,  1.2765e-02, -4.0271e-02],\n",
      "        [-1.6655e+00, -1.5249e+00, -1.9606e-01],\n",
      "        [ 5.3617e-01,  2.5399e-03,  1.3388e+00],\n",
      "        [ 1.5466e+00, -6.6703e-01,  3.3518e-01],\n",
      "        [ 3.1904e+00,  8.6668e-01, -9.0497e-02],\n",
      "        [-9.5961e-01,  6.2080e-01, -1.9145e-01],\n",
      "        [ 1.3016e+00, -2.6860e-02,  1.3234e+00]]) tensor([ 7.2694,  5.0125, -3.5275,  1.8693,  1.7118,  3.0186,  6.0611,  3.6762,\n",
      "        -4.1122,  4.6583])\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 10\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = Data.TensorDataset(features, labels)\n",
    "# 随机读取小批量\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "for X, y in data_iter:\n",
    "    print(X, y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(n_feature, 1)\n",
    "    # forward 定义前向传播\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "net = LinearNet(num_inputs)\n",
    "print(net) # 使用print可以打印出网络的结构\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 1: Sequential(\n",
      "  (0): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "method 2: Sequential(\n",
      "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "method 3: Sequential(\n",
      "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "Linear(in_features=3, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 写法一\n",
    "net1 = nn.Sequential(\n",
    "    nn.Linear(num_inputs, 1)\n",
    "    # 此处还可以传入其他层\n",
    "    )\n",
    "print('method 1:', net1)\n",
    "\n",
    "\n",
    "# 写法二\n",
    "net2 = nn.Sequential()\n",
    "net2.add_module('linear', nn.Linear(num_inputs, 1))\n",
    "print('method 2:', net2)\n",
    "# net.add_module ......\n",
    "\n",
    "# 写法三\n",
    "from collections import OrderedDict\n",
    "net = nn.Sequential(OrderedDict([\n",
    "          ('linear', nn.Linear(num_inputs, 1)),\n",
    "          # ......\n",
    "        ]))\n",
    "print('method 3:', net)\n",
    "\n",
    "print(net[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "init.normal_(net[0].weight, mean=0, std=0.01)\n",
    "init.constant_(net[0].bias, val=0)  # 也可以直接修改bias的data: net[0].bias.data.fill_(0)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "print(optimizer)\n",
    "\n",
    "# 调整学习率\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] *= 0.1 # 学习率为之前的0.1倍\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.002726\n",
      "epoch 2, loss: 0.003024\n",
      "epoch 3, loss: 0.002236\n",
      "epoch 4, loss: 0.001037\n",
      "epoch 5, loss: 0.000336\n",
      "epoch 6, loss: 0.000701\n",
      "epoch 7, loss: 0.000426\n",
      "epoch 8, loss: 0.000206\n",
      "epoch 9, loss: 0.000188\n",
      "epoch 10, loss: 0.000192\n",
      "epoch 11, loss: 0.000169\n",
      "epoch 12, loss: 0.000195\n",
      "epoch 13, loss: 0.000147\n",
      "epoch 14, loss: 0.000070\n",
      "epoch 15, loss: 0.000070\n",
      "epoch 16, loss: 0.000087\n",
      "epoch 17, loss: 0.000124\n",
      "epoch 18, loss: 0.000131\n",
      "epoch 19, loss: 0.000148\n",
      "epoch 20, loss: 0.000192\n",
      "epoch 21, loss: 0.000091\n",
      "epoch 22, loss: 0.000099\n",
      "epoch 23, loss: 0.000178\n",
      "epoch 24, loss: 0.000020\n",
      "epoch 25, loss: 0.000096\n",
      "epoch 26, loss: 0.000157\n",
      "epoch 27, loss: 0.000093\n",
      "epoch 28, loss: 0.000084\n",
      "epoch 29, loss: 0.000030\n",
      "epoch 30, loss: 0.000134\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        output = net(X)\n",
    "        l = loss(output, y.view(-1, 1))\n",
    "        optimizer.zero_grad() # 梯度清零，等价于net.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.05, -3.4, 1.28] Parameter containing:\n",
      "tensor([[ 2.0502, -3.3994,  1.2806]], requires_grad=True)\n",
      "0.2 Parameter containing:\n",
      "tensor([0.2002], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "dense = net[0]\n",
    "print(true_w, dense.weight)\n",
    "print(true_b, dense.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
